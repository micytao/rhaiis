# Install RHAIIS (Red Hat AI Inference Server) on OCP/K8s

> **âš ï¸ DISCLAIMER: Community Project**
> 
> This is **NOT an official Red Hat product or project**. This is a community-driven project intended **for experimentation and learning purposes only** with Red Hat AI Inference Server (RHAIIS).
> 
> **For official setup, implementation, and production deployments**, please refer to the official Red Hat documentation:
> ğŸ“– [Red Hat AI Inference Server Official Documentation](https://docs.redhat.com/en/documentation/red_hat_ai_inference_server/3.2/html/deploying_red_hat_ai_inference_server_in_openshift_container_platform/index)
> 
> **Use this project at your own risk** and ensure compliance with your organization's policies and Red Hat's licensing terms.

[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)
[![OpenShift](https://img.shields.io/badge/OpenShift-4.x-red.svg)](https://www.redhat.com/en/technologies/cloud-computing/openshift)
[![Kubernetes](https://img.shields.io/badge/Kubernetes-1.19+-blue.svg)](https://kubernetes.io/)
[![Community](https://img.shields.io/badge/Community-Experimental-orange.svg)](https://github.com/micytao/rhaiis)

A comprehensive deployment solution for Red Hat AI Inference Server (RHAIIS) that provides an OpenAI-compatible API server powered by vLLM for serving large language models with GPU acceleration.

## ğŸš€ Features

- **OpenAI-Compatible API**: Drop-in replacement for OpenAI API endpoints
- **GPU Acceleration**: Optimized for NVIDIA GPUs with vLLM backend
- **Multiple Deployment Options**: Helm charts and Kustomize configurations
- **Production Ready**: Comprehensive security, monitoring, and scaling features
- **Multi-Environment Support**: Development, staging, and production configurations
- **Operator Integration**: Automated GPU and Node Feature Discovery setup

## ğŸ“‹ Prerequisites

### Infrastructure Requirements
- **Kubernetes 1.19+** or **OpenShift 4.x**
- **GPU nodes** with NVIDIA GPUs
- **Persistent storage** (50Gi+ recommended for model cache)
- **Network access** to `registry.redhat.io` and `huggingface.co`

### Access Requirements
- **Red Hat Registry Access**: Valid credentials for `registry.redhat.io`
- **Hugging Face Token**: For model downloads ([Get token here](https://huggingface.co/settings/tokens))
- **Cluster Admin Privileges**: For operator installation (if using Kustomize)

### Software Requirements
- **Helm 3.2.0+** (for Helm deployment)
- **Kustomize** (included with kubectl 1.14+)
- **OpenShift CLI** or **kubectl**

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    RHAIIS Architecture                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚   Client    â”‚â”€â”€â”€â–¶â”‚ OpenShift    â”‚â”€â”€â”€â–¶â”‚   RHAIIS    â”‚    â”‚
â”‚  â”‚ Application â”‚    â”‚   Route      â”‚    â”‚    Pod      â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                               â”‚             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚              GPU Infrastructure             â”‚         â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚         â”‚  â”‚
â”‚  â”‚  â”‚     NFD     â”‚    â”‚  GPU Operator   â”‚    â–¼         â”‚  â”‚
â”‚  â”‚  â”‚  (Node      â”‚    â”‚  (GPU Resource  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚
â”‚  â”‚  â”‚ Discovery)  â”‚    â”‚   Management)   â”‚ â”‚  vLLM   â”‚  â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ Engine  â”‚  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”˜  â”‚
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                Storage Layer                        â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚   â”‚
â”‚  â”‚  â”‚ Model Cache â”‚         â”‚   Application Cache â”‚    â”‚   â”‚
â”‚  â”‚  â”‚   (20Gi)    â”‚         â”‚       (50Gi)        â”‚    â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Quick Start

### Option 1: Kustomize Deployment (Recommended)

```bash
# 1. Clone the repository
git clone https://github.com/micytao/rhaiis.git
cd rhaiis

# 2. Deploy operators first
kubectl apply -k kustomize/overlays/operators/install/

# 3. Wait for operators to be ready
kubectl wait --for=condition=Succeeded csv -n openshift-nfd --all --timeout=300s
kubectl wait --for=condition=Succeeded csv -n nvidia-gpu-operator --all --timeout=300s

# 4. Create operator instances
kubectl apply -k kustomize/overlays/operators/instances/

# 5. Deploy application
kubectl apply -k kustomize/overlays/app/
```

### Option 2: Helm Deployment

```bash
# 1. Clone the repository
git clone https://github.com/micytao/rhaiis.git
cd rhaiis

# 2. Create namespace and secrets
kubectl create namespace rhaiis
kubectl create secret docker-registry docker-secret \
  --docker-server=registry.redhat.io \
  --docker-username=<your-username> \
  --docker-password=<your-password> \
  --namespace=rhaiis

# 3. Install with Helm
helm install rhaiis ./rhaiis-helm-chart \
  --namespace rhaiis \
  --set secrets.huggingface.token="your-hf-token-here"
```

## ğŸ“ Project Structure

```
rhaiis/
â”œâ”€â”€ README.md                    # This file
â”œâ”€â”€ SETUP.md                     # Detailed setup instructions
â”œâ”€â”€ .gitignore                   # Git ignore rules (includes security exclusions)
â”‚
â”œâ”€â”€ kustomize/                  # ğŸ¯ Kustomize Deployment (Recommended)
â”‚   â”œâ”€â”€ README.md               # Kustomize-specific documentation
â”‚   â”œâ”€â”€ base/                   # Base Kubernetes resources
â”‚   â””â”€â”€ overlays/               # Environment-specific configurations
â”‚       â”œâ”€â”€ operators/          # GPU and NFD operator setup
â”‚       â”œâ”€â”€ dev/                # Development environment
â”‚       â”œâ”€â”€ prod/               # Production environment
â”‚       â””â”€â”€ app/                # Application overlay
â”‚
â”œâ”€â”€ rhaiis-helm-chart/          # ğŸ”§ Helm Chart
â”‚   â”œâ”€â”€ Chart.yaml              # Chart metadata
â”‚   â”œâ”€â”€ README.md               # Helm-specific documentation
â”‚   â”œâ”€â”€ INSTALL.md              # Quick installation guide
â”‚   â”œâ”€â”€ values.yaml             # Default configuration
â”‚   â”œâ”€â”€ values-local.yaml.template  # Template for local development
â”‚   â””â”€â”€ templates/              # Kubernetes manifests
â”‚       â”œâ”€â”€ deployment.yaml     # Main application deployment
â”‚       â”œâ”€â”€ service.yaml        # Service definition
â”‚       â”œâ”€â”€ route.yaml          # OpenShift route
â”‚       â”œâ”€â”€ secret.yaml         # Secret management
â”‚       â””â”€â”€ pvc-*.yaml          # Persistent volume claims
â”‚
â””â”€â”€ *.yml                       # ğŸ“„ Standalone manifests (legacy)
```

## ğŸ”§ Configuration Options

### Deployment Methods Comparison

| Feature | Kustomize | Helm Chart | Standalone |
|---------|-----------|------------|------------|
| **Ease of Use** | â­â­â­â­ | â­â­â­â­â­ | â­â­ |
| **Customization** | â­â­â­â­â­ | â­â­â­â­ | â­â­ |
| **Environment Management** | â­â­â­â­â­ | â­â­â­â­ | â­ |
| **Operator Integration** | â­â­â­â­â­ | â­â­â­ | â­ |
| **Production Ready** | â­â­â­â­â­ | â­â­â­â­â­ | â­â­ |
| **GitOps Friendly** | â­â­â­â­â­ | â­â­â­ | â­â­ |

### Key Configuration Parameters

| Parameter | Description | Default | Required |
|-----------|-------------|---------|----------|
| `app.model` | Hugging Face model to serve | `RedHatAI/Llama-3.2-1B-Instruct-FP8` | âœ… |
| `secrets.huggingface.token` | HF token for model access | `<placeholder>` | âœ… |
| `resources.limits.gpu` | GPU resource limit | `1` | âœ… |
| `storage.cache.size` | Application cache size | `50Gi` | âœ… |
| `storage.modelCache.size` | Model cache size | `20Gi` | âœ… |

## ğŸ” Security Best Practices

### Token Management

âš ï¸ **CRITICAL**: Never commit actual tokens to git!

This project implements multiple secure token handling methods:

1. **Command Line Injection** (Testing)
   ```bash
   helm install rhaiis ./rhaiis-helm-chart \
     --set secrets.huggingface.token="your-token"
   ```

2. **Local Values File** (Development)
   ```bash
   # Copy template and add your token
   cp rhaiis-helm-chart/values-local.yaml.template rhaiis-helm-chart/values-local.yaml
   # Edit values-local.yaml with your actual token (git-ignored)
   
   helm install rhaiis ./rhaiis-helm-chart \
     --values ./rhaiis-helm-chart/values-local.yaml
   ```

3. **External Secrets** (Production)
   ```bash
   # Create secret manually
   kubectl create secret generic my-hf-secret \
     --from-literal=HF_TOKEN="your-token" \
     --namespace rhaiis
   
   # Reference in deployment
   helm install rhaiis ./rhaiis-helm-chart \
     --set secrets.huggingface.existingSecret.name="my-hf-secret"
   ```

### Security Features

- âœ… **Git-ignored sensitive files** (`values-local.yaml`, `hf_token.txt`)
- âœ… **Non-root container execution**
- âœ… **Security contexts** with dropped capabilities
- âœ… **Network policies** (configurable)
- âœ… **RBAC** with minimal permissions
- âœ… **TLS termination** at route level

## ğŸŒ Environment-Specific Deployments

### Development Environment (Kustomize - Recommended)
```bash
# Lower resource requirements, single replica
kubectl apply -k kustomize/overlays/dev/
```

### Production Environment (Kustomize - Recommended)
```bash
# High availability, multiple replicas, production settings
kubectl apply -k kustomize/overlays/prod/
```

### Alternative: Helm Deployments

#### Development Environment
```bash
# Lower resource requirements, single replica
helm install rhaiis-dev ./rhaiis-helm-chart \
  --namespace rhaiis-dev \
  --values ./rhaiis-helm-chart/values-local.yaml \
  --set replicaCount=1 \
  --set resources.limits.cpu="4" \
  --set resources.limits.memory="8Gi"
```

#### Production Environment
```bash
# High availability, multiple replicas
helm install rhaiis-prod ./rhaiis-helm-chart \
  --namespace rhaiis-prod \
  --set replicaCount=3 \
  --set resources.limits.cpu="16" \
  --set resources.limits.memory="32Gi" \
  --set secrets.huggingface.existingSecret.name="prod-hf-secret"
```

## ğŸ“Š Monitoring and Observability

### Health Checks
- **Readiness Probe**: `/health` endpoint (30s delay, 10s interval)
- **Liveness Probe**: `/health` endpoint (60s delay, 30s interval)

### Metrics and Logging
- **Application Logs**: Available via `kubectl logs`
- **GPU Metrics**: Exposed via NVIDIA GPU Operator
- **Custom Metrics**: vLLM performance metrics (configurable)

### Verification Commands
```bash
# Check deployment status
kubectl get pods -n rhaiis
kubectl get routes -n rhaiis  # OpenShift
kubectl get ingress -n rhaiis # Kubernetes

# Test API endpoint
curl https://$(kubectl get route rhaiis-route -n rhaiis -o jsonpath='{.spec.host}')/health

# Check GPU allocation
kubectl describe node <gpu-node> | grep nvidia.com/gpu
```

## ğŸ”„ Scaling and Performance

### Horizontal Scaling
```bash
# Scale replicas
kubectl scale deployment rhaiis -n rhaiis --replicas=3

# Or via Helm upgrade
helm upgrade rhaiis ./rhaiis-helm-chart \
  --set replicaCount=3
```

### Vertical Scaling
```bash
# Increase resources
helm upgrade rhaiis ./rhaiis-helm-chart \
  --set resources.limits.cpu="32" \
  --set resources.limits.memory="64Gi" \
  --set resources.limits.nvidia.com/gpu="2"
```

### Performance Tuning
- **Model Parallelism**: Set `app.tensorParallelSize` for multi-GPU models
- **Cache Optimization**: Increase `storage.cache.size` for better performance
- **Memory Management**: Adjust `storage.sharedMemory.sizeLimit` for large models

## ğŸ› ï¸ Troubleshooting

### Common Issues

#### 1. GPU Not Available
```bash
# Check GPU operator status
kubectl get pods -n nvidia-gpu-operator
kubectl get clusterpolicy

# Verify GPU nodes
kubectl get nodes -l nvidia.com/gpu.present=true
```

#### 2. Model Download Failures
```bash
# Check HF token
kubectl get secret hf-secret -n rhaiis -o yaml

# Check network connectivity
kubectl exec -it <pod-name> -n rhaiis -- curl -I https://huggingface.co
```

#### 3. Pod Startup Issues
```bash
# Check pod events
kubectl describe pod <pod-name> -n rhaiis

# Check logs
kubectl logs <pod-name> -n rhaiis --previous
```

### Debug Mode
```bash
# Enable debug logging
helm upgrade rhaiis ./rhaiis-helm-chart \
  --set env.VLLM_LOG_LEVEL="DEBUG"
```

## ğŸ¤ Contributing

1. **Fork** the repository
2. **Create** a feature branch (`git checkout -b feature/amazing-feature`)
3. **Commit** your changes (`git commit -m 'Add amazing feature'`)
4. **Push** to the branch (`git push origin feature/amazing-feature`)
5. **Open** a Pull Request

### Development Setup
```bash
# Clone your fork
git clone https://github.com/your-username/rhaiis.git
cd rhaiis

# Create local values file
cp rhaiis-helm-chart/values-local.yaml.template rhaiis-helm-chart/values-local.yaml
# Edit with your tokens and preferences

# Test deployment
helm install rhaiis-dev ./rhaiis-helm-chart \
  --namespace rhaiis-dev \
  --values ./rhaiis-helm-chart/values-local.yaml \
  --dry-run --debug
```

## ğŸ“š Documentation

### Official Red Hat Documentation
- **[Red Hat AI Inference Server Official Documentation](https://docs.redhat.com/en/documentation/red_hat_ai_inference_server/3.2/html/deploying_red_hat_ai_inference_server_in_openshift_container_platform/index)** - Official Red Hat deployment guide

### Community Project Documentation
- **[Helm Chart Documentation](./rhaiis-helm-chart/README.md)** - Detailed Helm configuration
- **[Kustomize Documentation](./kustomize/README.md)** - Kustomize deployment guide
- **[Installation Guide](./rhaiis-helm-chart/INSTALL.md)** - Quick installation steps
- **[Setup Instructions](./SETUP.md)** - Security and token configuration

## ğŸ“„ License

This project is licensed under the Apache License 2.0 - see the [LICENSE](LICENSE) file for details.

## ğŸ†˜ Support

- **Issues**: [GitHub Issues](https://github.com/micytao/rhaiis/issues)
- **Discussions**: [GitHub Discussions](https://github.com/micytao/rhaiis/discussions)
- **Documentation**: [Project Wiki](https://github.com/micytao/rhaiis/wiki)

## ğŸ·ï¸ Tags

`#AI` `#LLM` `#vLLM` `#OpenShift` `#Kubernetes` `#GPU` `#RedHat` `#Helm` `#Kustomize` `#MLOps`

---

**Made with â¤ï¸ for the AI/ML community**
